1. Add causal mask for attention 
2. Add weight tying ???????
3. Use beam search
4. Use V100 to train, it is much faster than P100
6. BCELoss makes the model run much slower?????
7. Separate layernorm??
8. Rebuild the tokenzier and clean the report
9. INcrease max_length
10. Use contrastive loss?? -> No?
11. Add (1-lambda_init)
12. Check really hard the cleaning and tokenizing 
13. smaller topk?
14. Change visual extractor
15. maybe use a pretrained decoder for better caption
16. feed-forward layer 512-256?
17. Check GCA carefully
18. Check the metrics, use pycocoevalcap from CAMANet????
19. DROPOUT MAKES THE MODEL REPEAT BOS, PAD
20. Bigger model size, smaller learning rate
21. Use another version of numpy 
22. Skip evaluation breaks the deterministic training process
23. Train model with no keyword
24. Group by captions and evaluate per group to avoid repeated captions
25. Train with 0.2 dropout to avoid overfitting
26. Reduce number of epochs??????
27. Use greedy search for lower CIDEr score??
Ways to get better:
-Clean report
-Use beam search
-Change or fix binary classifier
-Tuning learning rate/delta


To fix:
-weight decay for 2d matrix only, not layernorm, ...
-cross_entropy loss
-add clip_grad_norm

Experiment with other lambda_init
Experiment with other SwiGlu and ReLu
